{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf34d2e",
   "metadata": {},
   "source": [
    "# La Banque Postale Fraud detector\n",
    "\n",
    "La phase de collecte (Collect) constitue la première étape de notre pipeline ETL. Son objectif est de rassembler toutes les sources de données nécessaires à l'analyse de détection de fraude. Dans ce notebook nous utilisons notamment le fichier de base https://huggingface.co/spaces/sdacelo/real-time-fraud-detection comme source initiale (inspiré de https://www.kaggle.com/datasets/kartik2112/fraud-detection).\n",
    "\n",
    "Principaux objectifs de cette étape :\n",
    "- Identifier et lister les sources de données (fichiers plats, bases, API, logs).\n",
    "- Extraire les données brutes sans altération.\n",
    "- Effectuer des vérifications de qualité basiques (présence de colonnes attendues, types, valeurs manquantes, doublons).\n",
    "- Appliquer des contrôles de confidentialité et sécurité (masquage ou exclusion si nécessaire).\n",
    "- Stocker les données brutes dans un emplacement dédié (par ex. dossier raw) avec méta‑informations (date d’extraction, source, checksum).\n",
    "\n",
    "Cette phase vise à garantir que les données d’entrées sont complètes et auditables avant d’entamer les étapes de transformation et de chargement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb104cfa",
   "metadata": {},
   "source": [
    "## Initialisation and upgrade the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6578462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Install and upgrade packages\n",
    "%pip install --upgrade pip --quiet\n",
    "%pip install mlflow boto3 psycopg2-binary pandas matplotlib plotly sqlalchemy scikit-learn asyncio asyncpg nbformat seaborn --quiet\n",
    "%pip install upgrade scikit-learn plotly pandas matplotlib sqlalchemy psycopg2-binary folium numpy --quiet\n",
    "\n",
    "# ==================== DATA MANIPULATION ====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "\n",
    "# ==================== VISUALIZATION ====================\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# ==================== MACHINE LEARNING ====================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay\n",
    ")\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# ==================== MLFLOW ====================\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# ==================== UTILITIES ====================\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from IPython.core.magic import register_cell_magic\n",
    "import platform\n",
    "\n",
    "# ==================== SETTINGS ====================\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f8993",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5677ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "debug = True  # True for debug mode (smaller dataset), False for full dataset\n",
    "samplingSize = 2000  # Number of rows to sample in debug mode, minimum 1000\n",
    "\n",
    "neonDB_connectionURL = 'postgresql://neondb_owner:npg_UIrY18vhNmLE@ep-curly-sound-ag9a7x4l-pooler.c-2.eu-central-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require'\n",
    "neonDB_fraudTableName = \"neondb\"\n",
    "HF_connectionURL = \"https://huggingface.co/spaces/sdacelo/real-time-fraud-detection\"\n",
    "HF_connectionCSV = \"https://lead-program-assets.s3.eu-west-3.amazonaws.com/M05-Projects/fraudTest.csv\"\n",
    "local_connectionURL = os.path.abspath(\"../datasSources/inputDataset/fraudTest.csv\")  # absolute path\n",
    "\n",
    "localDB_connectionURL = os.path.abspath(\"../datasSources/inputDataset/fraudTest.db\")  # absolute path\n",
    "localDB_tableName = \"transactions\"\n",
    "\n",
    "inputDBFormat = \"db\"  # \"csv\" or \"db\" or neon or HF_CSV\n",
    "\n",
    "dfRaw = pd.DataFrame()\n",
    "\n",
    "separator = (\"=\"*80)\n",
    "\n",
    "#---- Name defined by user for project ----\n",
    "\n",
    "modelPrefix = \"LBP_fraud_detector_\"\n",
    "EXPERIMENT_NAME = \"LBPFraudDetector\"\n",
    "\n",
    "\n",
    "#---- Jedha Colors for plots ----\n",
    "\n",
    "jedhaColor_violet = '#8409FF'\n",
    "jedhaColor_blue = '#3AE5FF'\n",
    "jedhaColor_blueLight = '#89C2FF'\n",
    "jedhaColor_white = '#DFF4F5'\n",
    "jedhaColor_black = '#170035'\n",
    "\n",
    "jedha_bg_color = jedhaColor_white\n",
    "jedha_grid_color = jedhaColor_black\n",
    "if platform.system() == \"Darwin\":\n",
    "    jedha_font = \"Avenir Next\"\n",
    "else:\n",
    "    jedha_font = \"Avenir Next, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol\"\n",
    "\n",
    "\n",
    "\n",
    "pio.templates[\"jedha_template\"] = go.layout.Template(\n",
    "    layout=go.Layout(\n",
    "        font=dict(family=jedha_font, color=jedhaColor_black),\n",
    "        title=dict(x=0.5, xanchor=\"center\", font=dict(size=24, color=jedhaColor_black)),\n",
    "        plot_bgcolor=jedha_bg_color,\n",
    "        paper_bgcolor=jedha_bg_color,\n",
    "        xaxis=dict(\n",
    "            gridcolor=jedha_grid_color,\n",
    "            zerolinecolor=jedha_grid_color,\n",
    "            linecolor=jedha_grid_color,\n",
    "            ticks=\"outside\",\n",
    "            tickcolor=jedha_grid_color,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            gridcolor=jedha_grid_color,\n",
    "            zerolinecolor=jedha_grid_color,\n",
    "            linecolor=jedha_grid_color,\n",
    "            ticks=\"outside\",\n",
    "            tickcolor=jedha_grid_color,\n",
    "        ),\n",
    "        legend=dict(\n",
    "            bgcolor=jedha_bg_color,\n",
    "            bordercolor=jedha_grid_color,\n",
    "            borderwidth=1,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "pio.templates.default = \"jedha_template\"\n",
    "\n",
    "colors = np.array([(132, 9, 255), (223,244,245), (58, 229, 255)])/255.\n",
    "jedhaCM = matplotlib.colors.LinearSegmentedColormap.from_list('Jedha Scale', colors)\n",
    "jedhaCMInverted = matplotlib.colors.LinearSegmentedColormap.from_list('Jedha Scale', colors)\n",
    "\n",
    "display(jedhaCM)\n",
    "\n",
    "\n",
    "# for compatibility with Jupyter Notebooks\n",
    "\n",
    "display(\"✅ Configuration load successful.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55fece",
   "metadata": {},
   "source": [
    "## User defined functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    \"\"\"A cell magic to skip execution of a cell.\n",
    "    Usage:\n",
    "    %%skip\n",
    "    <code to skip>\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "def logArrayToClipboard(array, array_name=\"Array\"):\n",
    "    \"\"\"Log a DataFrame or Series statistics to clipboard.   \n",
    "    Args:\n",
    "        array (_type_): _description_\n",
    "        array_name (str, optional): _description_. Defaults to \"Array\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # Export basic statistics to clipboard (Excel/Word friendly)\n",
    "    data_desc_rounded = array.round(2)\n",
    "    data_desc_rounded.to_clipboard(excel=True)\n",
    "    print(f\"✅ {array_name} copied to clipboard.\")\n",
    "\n",
    "def saveMap(df, nbPoint=None, outputPath=''):\n",
    "    \"\"\"Save a map with merchant locations and transaction clusters.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing transaction data.\n",
    "        nbPoint (int, optional): Number of points to plot. Defaults to None.\n",
    "        outputPath (str, optional): Path to save the map HTML file. Defaults to ''.\n",
    "    \"\"\"\n",
    "    # ~15min pour l'ensemble des points un fichier de 500mo\n",
    "    \n",
    "    # Center map on mean latitude and longitude of merchant locations\n",
    "    center_lat = df['merch_lat'].astype(float).mean()\n",
    "    center_lon = df['merch_long'].astype(float).mean()\n",
    "\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=5, tiles='CartoDB positron', control_scale=True, width='100%', height='100%', max_bounds=True)\n",
    "\n",
    "    # Add merchant locations as points\n",
    "    \n",
    "    if nbPoint:\n",
    "        dfTemp = df.head(nbPoint)\n",
    "    else:\n",
    "        dfTemp = df\n",
    "\n",
    "    # Group by merchant and count number of transactions and frauds\n",
    "    merchant_stats = dfTemp.groupby('merchant').agg(\n",
    "        total_transactions=('is_fraud', 'size'),\n",
    "        fraud_count=('is_fraud', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Draw points for merchant locations on the map\n",
    "    # Create separate marker clusters for fraud and legitimate transactions\n",
    "\n",
    "    fraud_cluster = MarkerCluster(name='Transactions frauduleuses').add_to(m)\n",
    "    legit_cluster = MarkerCluster(name='Transactions légitimes').add_to(m)\n",
    "\n",
    "    for idx, row in dfTemp.iterrows():\n",
    "        lat = float(row['merch_lat'])\n",
    "        lon = float(row['merch_long'])\n",
    "        merchant = row['merchant']\n",
    "        total_tx = merchant_stats.loc[merchant_stats['merchant'] == merchant, 'total_transactions'].values[0]\n",
    "        fraud_tx = merchant_stats.loc[merchant_stats['merchant'] == merchant, 'fraud_count'].values[0]\n",
    "        popup_text = (\n",
    "            f\"<b>Vendeur</b>: {merchant}<br>\"\n",
    "            f\"<b>Montant</b>: {row['amt']}$ <br>\"\n",
    "            f\"<b>Fraude</b>: {row['is_fraud']}<br>\"\n",
    "            f\"<b>Nombre total de transactions</b>: {total_tx}<br>\"\n",
    "            f\"<b>Nombre de transactions frauduleuses</b>: {fraud_tx}\"\n",
    "        )\n",
    "        if row['is_fraud'] == 1:\n",
    "            icon = folium.Icon(color='purple', icon='exclamation-sign', prefix='glyphicon')\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                popup=popup_text,\n",
    "                icon=icon\n",
    "            ).add_to(fraud_cluster)\n",
    "        else:\n",
    "            icon = folium.Icon(color='lightblue', icon='ok-sign', prefix='glyphicon')\n",
    "            folium.Marker(\n",
    "                location=[lat, lon],\n",
    "                popup=popup_text,\n",
    "                icon=icon\n",
    "            ).add_to(legit_cluster)\n",
    "\n",
    "    # Add layer control to toggle clusters\n",
    "    folium.LayerControl().add_to(m)\n",
    "\n",
    "    # Add legend to the map\n",
    "    legend_html = f'''\n",
    "     <div id=\"customLegend\" style=\"\n",
    "         position: fixed; \n",
    "         bottom: 50px; left: 50px; width: 200px; height: 90px; \n",
    "         background-color: white; z-index:9999; font-size:14px;\n",
    "         border:2px solid grey; border-radius:8px; padding: 10px;\">\n",
    "         <b>Légende</b><br>\n",
    "         <i class=\"glyphicon glyphicon-exclamation-sign\" style=\"color:{jedhaColor_violet}\"></i> Transaction frauduleuse<br>\n",
    "         <i class=\"glyphicon glyphicon-ok-sign\" style=\"color:{jedhaColor_blue}\"></i> Transaction légitime\n",
    "     </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "\n",
    "    m.save(outputPath, close_file=False)\n",
    "    display(f\"✅ Map saved to {outputPath}\")\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    Returns distance in kilometers\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371\n",
    "    return c * r\n",
    "\n",
    "def datetimeConverter(df, datetime_columns):\n",
    "    \"\"\"Convert specified columns in a DataFrame to datetime dtype.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the columns to convert.\n",
    "        datetime_columns (list): List of column names to convert to datetime.\n",
    "    \"\"\"\n",
    "    for col in datetime_columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                    print(f\"✓ {col}: converted to datetime64\")\n",
    "                else:\n",
    "                    print(f\"⊘ {col}: already datetime64\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ {col}: Failed to convert ({e})\")\n",
    "\n",
    "display(\"✅ UDF functions loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3de48",
   "metadata": {},
   "source": [
    "## Databases connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad95b3a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputDBFormat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m         dfRaw = pd.read_csv(HF_connectionCSV)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dfRaw\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m dfRaw = dataSourceLoader(\u001b[43minputDBFormat\u001b[49m)\n\u001b[32m     31\u001b[39m display(\u001b[33m\"\u001b[39m\u001b[33m✅ Dataframe successfully created from \u001b[39m\u001b[33m\"\u001b[39m + inputDBFormat + \u001b[33m\"\u001b[39m\u001b[33m format with \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dfRaw)) + \u001b[33m\"\u001b[39m\u001b[33m rows and \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dfRaw.columns)) + \u001b[33m\"\u001b[39m\u001b[33m columns.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'inputDBFormat' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def dataSourceLoader(inputDBFormat):\n",
    "    \"\"\"Load data from specified source format into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        inputDBFormat (str): The format of the data source (\"csv\", \"db\", \"neon\", \"HF\").\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    dfRaw = pd.DataFrame()\n",
    "\n",
    "    if inputDBFormat == \"csv\":\n",
    "        dfRaw = pd.read_csv(local_connectionURL)\n",
    "    elif inputDBFormat == \"db\":\n",
    "\n",
    "        conn = sqlite3.connect(localDB_connectionURL)\n",
    "        query = f\"SELECT * FROM {localDB_tableName}\"\n",
    "        dfRaw = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "    elif inputDBFormat == \"neon\":\n",
    "        engine = create_engine(neonDB_connectionURL)\n",
    "        query = f\"SELECT * FROM {neonDB_fraudTableName}\"\n",
    "        dfRaw = pd.read_sql_query(query, engine)\n",
    "    elif inputDBFormat == \"HF_CSV\":\n",
    "        dfRaw = pd.read_csv(HF_connectionCSV)\n",
    "\n",
    "    return dfRaw\n",
    "\n",
    "dfRaw = dataSourceLoader(inputDBFormat)\n",
    "\n",
    "display(\"✅ Dataframe successfully created from \" + inputDBFormat + \" format with \" + str(len(dfRaw)) + \" rows and \" + str(len(dfRaw.columns)) + \" columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61241b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DistanceCalculator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to calculate the great circle distance between customer and merchant locations.\n",
    "    \n",
    "    This transformer prevents data leakage by computing distances independently for each sample\n",
    "    without using any global statistics.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    None (stateless transformer)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None)\n",
    "        No-op operation as this transformer is stateless.\n",
    "    transform(X)\n",
    "        Calculates the distance between customer (lat, long) and merchant (merch_lat, merch_long).\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> pipeline = Pipeline([\n",
    "    ...     ('distance', DistanceCalculator()),\n",
    "    ...     ('classifier', RandomForestClassifier())\n",
    "    ... ])\n",
    "    >>> pipeline.fit(X_train, y_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method (no-op for stateless transformer).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features.\n",
    "        y : array-like, optional\n",
    "            Target variable (ignored).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self for method chaining.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Calculate distance between customer and merchant locations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features containing 'lat', 'long', 'merch_lat', 'merch_long' columns.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : pd.DataFrame\n",
    "            DataFrame with added 'distance_km' column.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        X['distance_km'] = X.apply(\n",
    "            lambda row: haversine(row['long'], row['lat'], row['merch_long'], row['merch_lat']), \n",
    "            axis=1\n",
    "        )\n",
    "        return X\n",
    "\n",
    "\n",
    "class AgeCalculator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to calculate customer age from date of birth.\n",
    "    \n",
    "    This transformer computes age at the time of transformation, ensuring\n",
    "    consistent age calculation for both training and test data.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    reference_date_ : pd.Timestamp\n",
    "        The reference date used for age calculation (set during fit).\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None)\n",
    "        Stores the reference date for age calculation.\n",
    "    transform(X)\n",
    "        Calculates age from 'dob' column using the stored reference date.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> pipeline = Pipeline([\n",
    "    ...     ('age', AgeCalculator()),\n",
    "    ...     ('classifier', RandomForestClassifier())\n",
    "    ... ])\n",
    "    >>> pipeline.fit(X_train, y_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Store reference date for consistent age calculation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features.\n",
    "        y : array-like, optional\n",
    "            Target variable (ignored).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self for method chaining.\n",
    "        \"\"\"\n",
    "        self.reference_date_ = pd.Timestamp.now()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Calculate customer age from date of birth.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features containing 'dob' column.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : pd.DataFrame\n",
    "            DataFrame with added 'age' column.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Ensure dob is datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(X['dob']):\n",
    "            X['dob'] = pd.to_datetime(X['dob'], errors='coerce')\n",
    "        \n",
    "        # Calculate age\n",
    "        X['age'] = (self.reference_date_ - X['dob']).dt.days // 365\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class TimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to extract temporal features from transaction datetime.\n",
    "    \n",
    "    Extracts hour, day of week, and month from transaction timestamp to capture\n",
    "    temporal patterns in fraudulent behavior.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    None (stateless transformer)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None)\n",
    "        No-op operation as this transformer is stateless.\n",
    "    transform(X)\n",
    "        Extracts time-based features from 'trans_date_trans_time' column.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> pipeline = Pipeline([\n",
    "    ...     ('time_features', TimeFeatureExtractor()),\n",
    "    ...     ('classifier', RandomForestClassifier())\n",
    "    ... ])\n",
    "    >>> pipeline.fit(X_train, y_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method (no-op for stateless transformer).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features.\n",
    "        y : array-like, optional\n",
    "            Target variable (ignored).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self for method chaining.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Extract temporal features from transaction datetime.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features containing 'trans_date_trans_time' column.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : pd.DataFrame\n",
    "            DataFrame with added 'trans_hour', 'trans_day', 'trans_month' columns.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Ensure datetime format\n",
    "        trans_dt = pd.to_datetime(X['trans_date_trans_time'], errors='coerce')\n",
    "        \n",
    "        # Extract temporal features\n",
    "        X['trans_hour'] = trans_dt.dt.hour\n",
    "        X['trans_day'] = trans_dt.dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "        X['trans_month'] = trans_dt.dt.month\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer to drop specified columns from DataFrame.\n",
    "    \n",
    "    This transformer removes columns that are not needed for modeling,\n",
    "    such as identifiers, raw datetime fields, or redundant features.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    columns_to_drop : list\n",
    "        List of column names to remove from the DataFrame.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None)\n",
    "        No-op operation as this transformer is stateless.\n",
    "    transform(X)\n",
    "        Removes specified columns from the DataFrame.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.pipeline import Pipeline\n",
    "    >>> dropper = ColumnDropper(['dob', 'trans_date_trans_time'])\n",
    "    >>> pipeline = Pipeline([\n",
    "    ...     ('drop_cols', dropper),\n",
    "    ...     ('classifier', RandomForestClassifier())\n",
    "    ... ])\n",
    "    >>> pipeline.fit(X_train, y_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns_to_drop):\n",
    "        \"\"\"\n",
    "        Initialize the ColumnDropper transformer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        columns_to_drop : list\n",
    "            List of column names to drop from the DataFrame.\n",
    "        \"\"\"\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit method (no-op for stateless transformer).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features.\n",
    "        y : array-like, optional\n",
    "            Target variable (ignored).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self for method chaining.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Drop specified columns from DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_transformed : pd.DataFrame\n",
    "            DataFrame with specified columns removed.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Only drop columns that exist in the DataFrame\n",
    "        existing_columns = [col for col in self.columns_to_drop if col in X.columns]\n",
    "        \n",
    "        if existing_columns:\n",
    "            X = X.drop(columns=existing_columns)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "print(\"✅ Custom transformers loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee2108",
   "metadata": {},
   "source": [
    "## EDA - Preparing datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset based on debug mode\n",
    "if debug:\n",
    "    # Use stratified sampling to ensure both fraud and non-fraud cases are included\n",
    "    fraud_cases = dfRaw[dfRaw['is_fraud'] == 1]\n",
    "    non_fraud_cases = dfRaw[dfRaw['is_fraud'] == 0]\n",
    "\n",
    "    # Calculate how many fraud cases to include (maintain approximate original ratio)\n",
    "    fraud_ratio = len(fraud_cases) / len(dfRaw)\n",
    "    n_fraud_samples = max(1, int(samplingSize * fraud_ratio))  # At least 1 fraud case\n",
    "    n_non_fraud_samples = samplingSize - n_fraud_samples\n",
    "\n",
    "    # Sample from each class\n",
    "    fraud_sample = fraud_cases.sample(n=min(n_fraud_samples, len(fraud_cases)), random_state=42)\n",
    "    non_fraud_sample = non_fraud_cases.sample(n=min(n_non_fraud_samples, len(non_fraud_cases)), random_state=42)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    df = pd.concat([fraud_sample, non_fraud_sample], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Using debug mode with {len(df)} rows and {len(df.columns)} columns (Fraud: {len(fraud_sample)}, Non-fraud: {len(non_fraud_sample)})\")\n",
    "else:\n",
    "    df = dfRaw\n",
    "    print(f\"Using full mode with {len(df)} rows and {len(df.columns)} columns .\")\n",
    "\n",
    "print()\n",
    "\n",
    "if debug:\n",
    "    # Display initial dataframe info\n",
    "    print(separator)\n",
    "    print(\"INITIAL DATAFRAME\")\n",
    "    print(separator)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print()\n",
    "    print(\"Columns in df:\")\n",
    "    display(df.columns.to_list())\n",
    "\n",
    "\n",
    "print(\"✅ Data preparation successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9aabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type Transformation and Optimization\n",
    "print(separator)\n",
    "print(\"TRANSFORMING DATAFRAME WITH OPTIMIZED DATA TYPES\")\n",
    "print(separator)\n",
    "print()\n",
    "\n",
    "# Store original memory usage\n",
    "original_memory = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "print(f\"Original memory usage: {original_memory:.2f} MB\")\n",
    "print()\n",
    "\n",
    "# Create a copy to transform\n",
    "dfOptimized = df.copy()\n",
    "\n",
    "# STEP 1: Handle datetime columns FIRST\n",
    "print(\"Converting datetime columns...\")\n",
    "\n",
    "# Define which columns are datetime columns\n",
    "datetime_columns = ['dob', 'trans_date_trans_time']\n",
    "\n",
    "datetimeConverter(df, ['dob'])\n",
    "datetimeConverter(df,  ['trans_date_trans_time'])\n",
    "\n",
    "print()\n",
    "\n",
    "# STEP 2: Apply other data type conversions\n",
    "type_conversions = {\n",
    "    # Transaction identifiers\n",
    "    'cc_num': 'int64',     # Credit card number as integer\n",
    "    \n",
    "    # Categorical columns with limited unique values\n",
    "    'merchant': 'category',\n",
    "    'category': 'category',\n",
    "    'job': 'category',\n",
    "    'gender': 'category',\n",
    "    'city': 'category',\n",
    "    'state': 'category',\n",
    "    \n",
    "    # Numeric columns - optimize size\n",
    "    'amt': 'float32',  # Transaction amount\n",
    "    'zip': 'int32',    # ZIP code\n",
    "    'city_pop': 'int32',  # Population\n",
    "    'unix_time': 'int64',  # Unix timestamp\n",
    "    \n",
    "    # Geographic coordinates\n",
    "    'lat': 'float32',\n",
    "    'long': 'float32',\n",
    "    'merch_lat': 'float32',\n",
    "    'merch_long': 'float32',\n",
    "    \n",
    "    # Boolean/Binary flags\n",
    "    'is_fraud': 'int8',  # 0 or 1\n",
    "}\n",
    "\n",
    "print(\"Applying data type conversions...\")\n",
    "print()\n",
    "\n",
    "# Apply conversions (skip datetime columns)\n",
    "for col, new_dtype in type_conversions.items():\n",
    "    if col not in dfOptimized.columns:\n",
    "        continue\n",
    "        \n",
    "    # Skip datetime columns\n",
    "    if col in datetime_columns:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Skip if already datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(dfOptimized[col]):\n",
    "            print(f\"⊘ {col}: Skipping (is datetime)\")\n",
    "            continue\n",
    "        \n",
    "        old_dtype = dfOptimized[col].dtype\n",
    "        \n",
    "        if new_dtype == 'category':\n",
    "            dfOptimized[col] = dfOptimized[col].astype('category')\n",
    "        elif new_dtype in ['float32', 'float64']:\n",
    "            dfOptimized[col] = pd.to_numeric(dfOptimized[col], errors='coerce').astype(np.dtype(new_dtype))\n",
    "        elif new_dtype in ['int8', 'int16', 'int32', 'int64']:\n",
    "            # For integer conversions, fill NaN with 0 first\n",
    "            dfOptimized[col] = pd.to_numeric(dfOptimized[col], errors='coerce').fillna(0).astype(np.dtype(new_dtype))\n",
    "        else:\n",
    "            dfOptimized[col] = dfOptimized[col].astype(np.dtype(new_dtype))\n",
    "        \n",
    "        print(f\"✓ {col}: {old_dtype} → {new_dtype}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {col}: Failed - {str(e)[:80]}\")\n",
    "\n",
    "print()\n",
    "print(separator)\n",
    "print(\"TRANSFORMATION COMPLETE\")\n",
    "print(separator)\n",
    "print()\n",
    "\n",
    "# Calculate new memory usage\n",
    "optimized_memory = dfOptimized.memory_usage(deep=True).sum() / (1024**2)\n",
    "memory_saved = original_memory - optimized_memory\n",
    "memory_saved_pct = (memory_saved / original_memory) * 100 if original_memory > 0 else 0\n",
    "\n",
    "print(f\"Original memory usage:  {original_memory:.2f} MB\")\n",
    "print(f\"Optimized memory usage: {optimized_memory:.2f} MB\")\n",
    "print(f\"Memory saved:           {memory_saved:.2f} MB ({memory_saved_pct:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Display new data types\n",
    "print(separator)\n",
    "print(\"NEW DATA TYPES:\")\n",
    "print(separator)\n",
    "print(dfOptimized.dtypes)\n",
    "print()\n",
    "\n",
    "# Verify data integrity\n",
    "print(separator)\n",
    "print(\"DATA INTEGRITY CHECK:\")\n",
    "print(separator)\n",
    "print(f\"Original shape:  {df.shape}\")\n",
    "print(f\"Optimized shape: {dfOptimized.shape}\")\n",
    "print(f\"Fraud count (original):  {df['is_fraud'].sum()}\")\n",
    "print(f\"Fraud count (optimized): {dfOptimized['is_fraud'].sum()}\")\n",
    "print()\n",
    "\n",
    "# Replace the original dataframe\n",
    "df = dfOptimized\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print(\"✅ Dataframe successfully optimized and updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b294584",
   "metadata": {},
   "source": [
    "## EDA - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(separator)\n",
    "print(\"EXPLORATORY DATA ANALYSIS (EDA)\")\n",
    "print(separator)\n",
    "print()\n",
    "\n",
    "\n",
    "def Preprocessor(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the DataFrame for exploratory data analysis (EDA).\n",
    "    \n",
    "    This function performs feature engineering and data cleanup to prepare\n",
    "    the dataset for visualization and analysis. It calculates derived features\n",
    "    such as distance, age, and temporal features, then removes columns that\n",
    "    are no longer needed.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to preprocess.   \n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame ready for EDA.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the distance between customer and merchant locations\n",
    "    df['distance_km'] = df.apply(\n",
    "        lambda row: haversine(row['long'], row['lat'], row['merch_long'], row['merch_lat']), \n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"Distance calculated. Min: {df['distance_km'].min():.2f} km, \"\n",
    "          f\"Max: {df['distance_km'].max():.2f} km, \"\n",
    "          f\"Mean: {df['distance_km'].mean():.2f} km\")\n",
    "\n",
    "    # Convert dob to datetime and calculate age\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['dob']):\n",
    "        df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "\n",
    "    df['age'] = (pd.Timestamp.now() - df['dob']).dt.days // 365\n",
    "    df = df.sort_values(by='age', ascending=True)\n",
    "\n",
    "    # Convert amt to numeric\n",
    "    df['amt'] = pd.to_numeric(df['amt'], errors='coerce')\n",
    "    \n",
    "    # Extract hour from transaction datetime\n",
    "    df['trans_hour'] = pd.to_datetime(df['trans_date_trans_time']).dt.hour\n",
    "\n",
    "    # Drop columns that are no longer needed (only if they exist)\n",
    "    columns_to_drop = [\n",
    "        'dob', 'trans_date_trans_time', 'unix_time', \n",
    "        'lat', 'long', 'merch_lat', 'merch_long', \n",
    "        'cc_num', 'street', 'first', 'last', 'Column1', 'trans_num', \"unamed: 0\"\n",
    "    ]\n",
    "    existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "    \n",
    "    if existing_columns_to_drop:\n",
    "        df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "        print(f\"Dropped columns: {existing_columns_to_drop}\")\n",
    "    else:\n",
    "        print(\"No columns to drop (already removed or not present).\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "df = Preprocessor(df)\n",
    "    \n",
    "print(\"✅ Final cleanup before EDA complete.\")\n",
    "print()\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "print(\"Data types of key columns:\")\n",
    "print(f\"  age: {df['age'].dtype}\")\n",
    "print(f\"  amt: {df['amt'].dtype}\")\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"✅ EDA analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15950a",
   "metadata": {},
   "source": [
    "## Plotting distribution, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "# Univariate analysis - Distribution of numeric variables\n",
    "print(\"Generating distributions for numeric features...\")\n",
    "num_features = [\"age\", \"amt\", \"trans_hour\", \"distance_km\", \"city_pop\"]\n",
    "\n",
    "for f in num_features:\n",
    "    fig = px.histogram(\n",
    "        df.head(samplingSize).sort_values(by=f, ascending=True), \n",
    "        x=f,\n",
    "        title=f'Distribution of {f}'\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Distribution of transaction amounts by fraud status\n",
    "\n",
    "print(\"Distribution des fraudes:\")\n",
    "print(df['is_fraud'].value_counts())\n",
    "print()\n",
    "#print(df['is_fraud'].describe())\n",
    "print()\n",
    "\n",
    "# Visualize transaction amounts: Normal vs Fraudulent\n",
    "fig = go.Figure()\n",
    "# Correlation heatmap for numeric features\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "corr = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=jedhaCMInverted, square=True)\n",
    "plt.title(\"Heatmap de corrélation des variables numériques\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normal transactions\n",
    "\"\"\" fig.add_trace(go.Histogram(\n",
    "    x=df[df['is_fraud']==0]['amt'],\n",
    "    name='Transactions conformes',\n",
    "    nbinsx=50,\n",
    "    opacity=0.7,\n",
    "    marker_color=jedhaColor_blue\n",
    ")) \"\"\"\n",
    "\n",
    "# Fraud transactions\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df[df['is_fraud']==1]['amt'],\n",
    "    name='Transactions frauduleuses',\n",
    "    nbinsx=50,\n",
    "    opacity=0.7,\n",
    "    marker_color=jedhaColor_violet\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution des montants de transactions: Conformes vs Frauduleuses',\n",
    "    xaxis_title='Montant',\n",
    "    yaxis_title='Fréquence',\n",
    "    barmode='overlay',\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize fraud distribution by hour\n",
    "fig = go.Figure()\n",
    "\n",
    "# Normal transactions by hour\n",
    "\"\"\" fig.add_trace(go.Histogram(\n",
    "    x=df[df['is_fraud']==0]['trans_hour'],\n",
    "    name='Transactions conformes',\n",
    "    nbinsx=24,\n",
    "    opacity=0.7,\n",
    "    marker_color=jedhaColor_blue\n",
    ")) \"\"\"\n",
    "\n",
    "# Fraud transactions by hour\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df[df['is_fraud']==1]['trans_hour'],\n",
    "    name='Transactions frauduleuses',\n",
    "    nbinsx=24,\n",
    "    opacity=0.7,\n",
    "    marker_color=jedhaColor_violet\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution des transactions par heure: Conformes vs Frauduleuses',\n",
    "    xaxis_title='Heure de la journée',\n",
    "    yaxis_title='Fréquence',\n",
    "    barmode='overlay',\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Visualize fraud distribution by category\n",
    "fig = go.Figure()\n",
    "\n",
    "# Fraud transactions by category\n",
    "fraud_by_category = df[df['is_fraud']==1].groupby('category').size().reset_index(name='count')\n",
    "fraud_by_category = fraud_by_category.sort_values('count', ascending=False)\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=fraud_by_category['category'],\n",
    "    y=fraud_by_category['count'],\n",
    "    name='Transactions frauduleuses',\n",
    "    marker_color=jedhaColor_violet\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Distribution des transactions frauduleuses par catégorie',\n",
    "    xaxis_title='Catégorie',\n",
    "    yaxis_title='Nombre de fraudes',\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "\n",
    "# Add Pareto curve (cumulative percentage)\n",
    "fraud_by_category['cumulative'] = fraud_by_category['count'].cumsum()\n",
    "fraud_by_category['cumulative_pct'] = fraud_by_category['cumulative'] / fraud_by_category['count'].sum() * 100\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=fraud_by_category['category'],\n",
    "    y=fraud_by_category['cumulative_pct'],\n",
    "    name='Courbe de Pareto (%)',\n",
    "    mode='lines+markers',\n",
    "    marker_color=jedhaColor_blue,\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis2=dict(\n",
    "        title='Pourcentage cumulatif (%)',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        range=[0, 100],\n",
    "        showgrid=False,\n",
    "\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "print(\"✅ Fraud distribution analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5ac11",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183024bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(separator)\n",
    "print(\"MODEL TRAINING AND EVALUATION\")\n",
    "print(separator)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Prepare Data for Modeling\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Separating labels from features...\")\n",
    "\n",
    "# Separate target variable Y from features X\n",
    "X = df.drop(columns=[\"is_fraud\"], inplace=False)\n",
    "Y = df[\"is_fraud\"]\n",
    "\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Unique classes in Y: {Y.unique()}\")\n",
    "print()\n",
    "\n",
    "# Check if we have sufficient samples for stratified split\n",
    "if len(Y.unique()) < 2:\n",
    "    print(\"⚠️ ERROR: Only one class present in target variable. Cannot train models.\")\n",
    "    print(f\"Please increase samplingSize to ensure at least 20 fraud cases.\")\n",
    "else:\n",
    "    min_samples_per_class = Y.value_counts().min()\n",
    "    \n",
    "    if min_samples_per_class < 2:\n",
    "        print(f\"⚠️ ERROR: Insufficient samples for stratified split.\")\n",
    "        print(f\"Minimum samples per class: {min_samples_per_class}\")\n",
    "    else:\n",
    "        print(\"✓ Sufficient samples for stratified split\")\n",
    "        print(f\"Fraud cases: {Y.sum()}, Non-fraud cases: {len(Y) - Y.sum()}\")\n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 2: Train/Test Split\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Splitting data into training and testing sets...\")\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            X, Y, \n",
    "            test_size=0.2, \n",
    "            random_state=42, \n",
    "            stratify=Y\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set size: {X_train.shape[0]} (Fraud: {Y_train.sum()}, Non-fraud: {len(Y_train) - Y_train.sum()})\")\n",
    "        print(f\"Test set size: {X_test.shape[0]} (Fraud: {Y_test.sum()}, Non-fraud: {len(Y_test) - Y_test.sum()})\")\n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 3: Prepare Features for Modeling\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Encoding features for machine learning...\")\n",
    "        \n",
    "        # Prepare features\n",
    "        X_train_processed = X_train.copy()\n",
    "        X_test_processed = X_test.copy()\n",
    "        \n",
    "        # Convert datetime to numeric if exists (days since epoch)\n",
    "        if 'dob' in X_train_processed.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(X_train_processed['dob']):\n",
    "                X_train_processed['dob'] = (X_train_processed['dob'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "                X_test_processed['dob'] = (X_test_processed['dob'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "        \n",
    "        # Encode categorical features with handling for unseen labels\n",
    "        label_encoders = {}\n",
    "        unknown_counts = {}\n",
    "        \n",
    "        for col in X_train_processed.columns:\n",
    "            if X_train_processed[col].dtype == 'category' or X_train_processed[col].dtype == 'object':\n",
    "                # Fit encoder on training data\n",
    "                label_encoders[col] = LabelEncoder()\n",
    "                X_train_processed[col] = label_encoders[col].fit_transform(X_train_processed[col].astype(str))\n",
    "                \n",
    "                # Transform test data with handling for unseen labels\n",
    "                # Get unique values in test set\n",
    "                test_values = X_test_processed[col].astype(str)\n",
    "                \n",
    "                # Find values in test that weren't in train\n",
    "                unseen_mask = ~test_values.isin(label_encoders[col].classes_)\n",
    "                unseen_count = unseen_mask.sum()\n",
    "                \n",
    "                if unseen_count > 0:\n",
    "                    unknown_counts[col] = unseen_count\n",
    "                    # Replace unseen values with the most common value from training\n",
    "                    most_common_value = X_train[col].mode()[0] if len(X_train[col].mode()) > 0 else label_encoders[col].classes_[0]\n",
    "                    test_values[unseen_mask] = str(most_common_value)\n",
    "                \n",
    "                # Transform test data\n",
    "                X_test_processed[col] = label_encoders[col].transform(test_values)\n",
    "        \n",
    "        # Report unknown categories\n",
    "        if unknown_counts:\n",
    "            print(f\"\\n⚠️ Unknown categories found in test set:\")\n",
    "            for col, count in unknown_counts.items():\n",
    "                print(f\"  {col}: {count} unseen values (replaced with most common training value)\")\n",
    "            print()\n",
    "        \n",
    "        # Ensure all features are numeric\n",
    "        X_train_processed = X_train_processed.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        X_test_processed = X_test_processed.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        print(f\"✓ Features encoded\")\n",
    "        print(f\"  Train shape: {X_train_processed.shape}\")\n",
    "        print(f\"  Test shape: {X_test_processed.shape}\")\n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 4: Train Multiple Models\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Training models...\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Define models to test\n",
    "        models_to_train = {\n",
    "            'LogisticRegression': LogisticRegression(max_iter=100, random_state=42, class_weight='balanced'),\n",
    "            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "            'SVC': SVC(kernel='rbf', random_state=42, class_weight='balanced', probability=True)\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in models_to_train.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Train the model\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_processed, Y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Make predictions\n",
    "            y_train_pred = model.predict(X_train_processed)\n",
    "            y_test_pred = model.predict(X_test_processed)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_accuracy = accuracy_score(Y_train, y_train_pred)\n",
    "            test_accuracy = accuracy_score(Y_test, y_test_pred)\n",
    "            train_f1 = f1_score(Y_train, y_train_pred)\n",
    "            test_f1 = f1_score(Y_test, y_test_pred)\n",
    "            \n",
    "            # Store results\n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'train_accuracy': train_accuracy,\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'train_f1': train_f1,\n",
    "                'test_f1': test_f1,\n",
    "                'train_time': train_time,\n",
    "                'y_train_pred': y_train_pred,\n",
    "                'y_test_pred': y_test_pred\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Training time: {train_time:.2f}s\")\n",
    "            print(f\"Train Accuracy: {train_accuracy:.4f} | Test Accuracy: {test_accuracy:.4f}\")\n",
    "            print(f\"Train F1 Score: {train_f1:.4f} | Test F1 Score: {test_f1:.4f}\")\n",
    "            \n",
    "            # Visualize confusion matrices for this model\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Training confusion matrix\n",
    "            ConfusionMatrixDisplay.from_predictions(\n",
    "                Y_train, y_train_pred, ax=ax1, cmap=jedhaCM\n",
    "            )\n",
    "            ax1.set_title(f\"{model_name} - Training Set\", color=jedhaColor_black, fontsize=12, fontweight='bold')\n",
    "            ax1.set_facecolor(jedha_bg_color)\n",
    "            ax1.xaxis.label.set_color(jedhaColor_black)\n",
    "            ax1.yaxis.label.set_color(jedhaColor_black)\n",
    "            ax1.tick_params(colors=jedhaColor_black)\n",
    "            \n",
    "            # Test confusion matrix\n",
    "            ConfusionMatrixDisplay.from_predictions(\n",
    "                Y_test, y_test_pred, ax=ax2, cmap=jedhaCM\n",
    "            )\n",
    "            ax2.set_title(f\"{model_name} - Test Set\", color=jedhaColor_black, fontsize=12, fontweight='bold')\n",
    "            ax2.set_facecolor(jedha_bg_color)\n",
    "            ax2.xaxis.label.set_color(jedhaColor_black)\n",
    "            ax2.yaxis.label.set_color(jedhaColor_black)\n",
    "            ax2.tick_params(colors=jedhaColor_black)\n",
    "            \n",
    "            fig.patch.set_facecolor(jedha_bg_color)\n",
    "            fig.suptitle(f'{model_name} - Confusion Matrices', \n",
    "                        fontsize=14, fontweight='bold', color=jedhaColor_black, y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\n\" + separator)\n",
    "        print(\"✅ Model training complete!\")\n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 5: Model Comparison\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"\\n\" + separator)\n",
    "        print(\"MODEL COMPARISON\")\n",
    "        print(separator)\n",
    "        print()\n",
    "        \n",
    "        # Create comparison dataframe\n",
    "        results_df = pd.DataFrame({\n",
    "            'Model': list(results.keys()),\n",
    "            'Train Accuracy': [r['train_accuracy'] for r in results.values()],\n",
    "            'Test Accuracy': [r['test_accuracy'] for r in results.values()],\n",
    "            'Train F1': [r['train_f1'] for r in results.values()],\n",
    "            'Test F1': [r['test_f1'] for r in results.values()],\n",
    "            'Time (s)': [r['train_time'] for r in results.values()]\n",
    "        })\n",
    "        \n",
    "        # Sort by Test F1 Score\n",
    "        results_df = results_df.sort_values('Test F1', ascending=False)\n",
    "        \n",
    "        display(results_df)\n",
    "        print()\n",
    "        \n",
    "        # Visualize model comparison\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.patch.set_facecolor(jedha_bg_color)\n",
    "        \n",
    "        # 1. Accuracy Comparison (Train vs Test)\n",
    "        ax = axes[0, 0]\n",
    "        x = np.arange(len(results_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, results_df['Train Accuracy'], width, \n",
    "               label='Train', color=jedhaColor_blue, alpha=0.8)\n",
    "        ax.bar(x + width/2, results_df['Test Accuracy'], width, \n",
    "               label='Test', color=jedhaColor_violet, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Model', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_ylabel('Accuracy', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_title('Accuracy Comparison: Train vs Test', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(results_df['Model'], rotation=15, ha='right')\n",
    "        ax.legend(facecolor=jedha_bg_color, edgecolor=jedhaColor_black)\n",
    "        ax.set_facecolor(jedha_bg_color)\n",
    "        ax.tick_params(colors=jedhaColor_black)\n",
    "        ax.grid(True, alpha=0.3, color=jedhaColor_black)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(jedhaColor_black)\n",
    "        \n",
    "        # 2. F1 Score Comparison (Train vs Test)\n",
    "        ax = axes[0, 1]\n",
    "        ax.bar(x - width/2, results_df['Train F1'], width, \n",
    "               label='Train', color=jedhaColor_blue, alpha=0.8)\n",
    "        ax.bar(x + width/2, results_df['Test F1'], width, \n",
    "               label='Test', color=jedhaColor_violet, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Model', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_ylabel('F1 Score', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_title('F1 Score Comparison: Train vs Test', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(results_df['Model'], rotation=15, ha='right')\n",
    "        ax.legend(facecolor=jedha_bg_color, edgecolor=jedhaColor_black)\n",
    "        ax.set_facecolor(jedha_bg_color)\n",
    "        ax.tick_params(colors=jedhaColor_black)\n",
    "        ax.grid(True, alpha=0.3, color=jedhaColor_black)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(jedhaColor_black)\n",
    "        \n",
    "        # 3. Overfitting Detection (Accuracy Gap)\n",
    "        ax = axes[1, 0]\n",
    "        accuracy_gap = results_df['Train Accuracy'] - results_df['Test Accuracy']\n",
    "        colors_gap = [jedhaColor_violet if gap > 0.05 else jedhaColor_blue for gap in accuracy_gap]\n",
    "        \n",
    "        ax.bar(results_df['Model'], accuracy_gap, color=colors_gap, alpha=0.8)\n",
    "        ax.axhline(y=0, color=jedhaColor_black, linestyle='-', linewidth=0.5)\n",
    "        ax.axhline(y=0.05, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Overfitting threshold')\n",
    "        \n",
    "        ax.set_xlabel('Model', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_ylabel('Train - Test Accuracy', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_title('Overfitting Detection (Accuracy Gap)', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_xticklabels(results_df['Model'], rotation=15, ha='right')\n",
    "        ax.legend(facecolor=jedha_bg_color, edgecolor=jedhaColor_black)\n",
    "        ax.set_facecolor(jedha_bg_color)\n",
    "        ax.tick_params(colors=jedhaColor_black)\n",
    "        ax.grid(True, alpha=0.3, color=jedhaColor_black)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(jedhaColor_black)\n",
    "        \n",
    "        # 4. Training Time Comparison\n",
    "        ax = axes[1, 1]\n",
    "        ax.bar(results_df['Model'], results_df['Time (s)'], color=jedhaColor_blue, alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Model', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_ylabel('Training Time (seconds)', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_title('Training Time Comparison', fontweight='bold', color=jedhaColor_black)\n",
    "        ax.set_xticklabels(results_df['Model'], rotation=15, ha='right')\n",
    "        ax.set_facecolor(jedha_bg_color)\n",
    "        ax.tick_params(colors=jedhaColor_black)\n",
    "        ax.grid(True, alpha=0.3, color=jedhaColor_black)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(jedhaColor_black)\n",
    "        \n",
    "        plt.suptitle('Model Performance Comparison', \n",
    "                    fontsize=16, fontweight='bold', color=jedhaColor_black, y=0.995)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 6: Best Model Analysis\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"BEST MODEL DETAILED ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        best_model_name = results_df.iloc[0]['Model']\n",
    "        best_result = results[best_model_name]\n",
    "        \n",
    "        print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "        print(f\"   Test F1 Score: {best_result['test_f1']:.4f}\")\n",
    "        print(f\"   Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # ROC Curve for best model\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        RocCurveDisplay.from_estimator(\n",
    "            best_result['model'], \n",
    "            X_test_processed, \n",
    "            Y_test, \n",
    "            ax=ax, \n",
    "            color=jedhaColor_violet,\n",
    "            lw=3\n",
    "        )\n",
    "        ax.set_facecolor(jedha_bg_color)\n",
    "        fig.patch.set_facecolor(jedha_bg_color)\n",
    "        ax.set_title(f\"ROC Curve - {best_model_name} (Test Set)\", \n",
    "                    fontsize=14, fontweight='bold', color=jedhaColor_black)\n",
    "        ax.xaxis.label.set_color(jedhaColor_black)\n",
    "        ax.yaxis.label.set_color(jedhaColor_black)\n",
    "        ax.tick_params(colors=jedhaColor_black)\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(jedhaColor_black)\n",
    "        ax.legend(facecolor=jedha_bg_color, labelcolor=jedhaColor_black)\n",
    "        ax.grid(True, alpha=0.3, color=jedhaColor_black)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification Report\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(classification_report(Y_test, best_result['y_test_pred'], \n",
    "                                   target_names=['Non-Fraud', 'Fraud']))\n",
    "        \n",
    "        # Feature importance (if RandomForest)\n",
    "        if best_model_name == 'RandomForest':\n",
    "            print(\"\\nTop 10 Most Important Features:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_train_processed.columns,\n",
    "                'importance': best_result['model'].feature_importances_\n",
    "            }).sort_values('importance', ascending=False).head(10)\n",
    "            \n",
    "            display(feature_importance)\n",
    "            \n",
    "            # Visualize feature importance\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.barh(feature_importance['feature'], feature_importance['importance'], \n",
    "                   color=jedhaColor_violet, alpha=0.8)\n",
    "            ax.set_xlabel('Importance', fontweight='bold', color=jedhaColor_black)\n",
    "            ax.set_ylabel('Feature', fontweight='bold', color=jedhaColor_black)\n",
    "            ax.set_title('Top 10 Feature Importances - RandomForest', \n",
    "                        fontweight='bold', color=jedhaColor_black)\n",
    "            ax.set_facecolor(jedha_bg_color)\n",
    "            fig.patch.set_facecolor(jedha_bg_color)\n",
    "            ax.tick_params(colors=jedhaColor_black)\n",
    "            ax.grid(True, alpha=0.3, color=jedhaColor_black, axis='x')\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_color(jedhaColor_black)\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\n✅ Model evaluation complete!\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914cf0e",
   "metadata": {},
   "source": [
    "## Creation and upload Experiment for mlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cfa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(separator)\n",
    "print(\"MLFLOW EXPERIMENT TRACKING\")\n",
    "print(separator)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MLflow Configuration and Experiment Setup\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# Set tracking URI (MLflow server)\n",
    "#mlflow.set_tracking_uri(\"http://localhost:4000/\")\n",
    "mlflow.set_tracking_uri(\"https://davidrambeau-mlflow.hf.space/\")\n",
    "\n",
    "# Set experiment info\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Get experiment metadata\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"Experiment Name: {EXPERIMENT_NAME}\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# Log Best Model to MLflow\n",
    "# ============================================================================\n",
    "\n",
    "if 'best_model_name' in locals() and 'best_result' in locals():\n",
    "    \n",
    "    print(f\"Logging best model to MLflow: {best_model_name}\")\n",
    "    print()\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id, run_name=f\"{best_model_name}_production\") as run:\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Log Parameters\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Logging parameters...\")\n",
    "        \n",
    "        # Get model parameters\n",
    "        model_params = best_result['model'].get_params()\n",
    "        \n",
    "        # Log general parameters\n",
    "        mlflow.log_param(\"model_type\", best_model_name)\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        mlflow.log_param(\"debug_mode\", debug)\n",
    "        mlflow.log_param(\"sampling_size\", samplingSize if debug else len(dfRaw))\n",
    "        \n",
    "        # Log model-specific parameters\n",
    "        for param_name, param_value in model_params.items():\n",
    "            try:\n",
    "                # Skip complex objects\n",
    "                if isinstance(param_value, (int, float, str, bool)) or param_value is None:\n",
    "                    mlflow.log_param(f\"model_{param_name}\", param_value)\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Could not log parameter {param_name}: {e}\")\n",
    "        \n",
    "        print(\"✓ Parameters logged\")\n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Log Metrics\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Logging metrics...\")\n",
    "        \n",
    "        # Training metrics\n",
    "        mlflow.log_metric(\"train_accuracy\", best_result['train_accuracy'])\n",
    "        mlflow.log_metric(\"train_f1_score\", best_result['train_f1'])\n",
    "        \n",
    "        # Test metrics (most important)\n",
    "        mlflow.log_metric(\"test_accuracy\", best_result['test_accuracy'])\n",
    "        mlflow.log_metric(\"test_f1_score\", best_result['test_f1'])\n",
    "        \n",
    "        # Performance metrics\n",
    "        mlflow.log_metric(\"training_time_seconds\", best_result['train_time'])\n",
    "        \n",
    "        # Dataset metrics\n",
    "        mlflow.log_metric(\"total_samples\", len(X))\n",
    "        mlflow.log_metric(\"train_samples\", len(X_train))\n",
    "        mlflow.log_metric(\"test_samples\", len(X_test))\n",
    "        mlflow.log_metric(\"fraud_ratio\", Y.sum() / len(Y))\n",
    "        \n",
    "        print(\"✓ Metrics logged\")\n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Log Model\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Logging model artifact...\")\n",
    "        \n",
    "        try:\n",
    "            # Create predictions for signature inference\n",
    "            predictions = best_result['model'].predict(X_train_processed)\n",
    "            \n",
    "            # Infer model signature\n",
    "            signature = infer_signature(X_train_processed, predictions)\n",
    "            \n",
    "            # Log the model\n",
    "            mlflow.sklearn.log_model(\n",
    "                sk_model=best_result['model'],\n",
    "                name=\"fraud_detector_model\",\n",
    "                registered_model_name=f\"fraud_detector_{best_model_name}\",\n",
    "                signature=signature,\n",
    "                input_example=X_train_processed.head(5)\n",
    "            )\n",
    "            \n",
    "            print(\"✓ Model artifact logged\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not log model artifact: {e}\")\n",
    "            print(\"  This is usually due to S3 bucket permissions or connectivity issues.\")\n",
    "            print(\"  Metrics and parameters were still logged successfully.\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Log Additional Artifacts\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Logging additional artifacts...\")\n",
    "        \n",
    "        try:\n",
    "            # Save feature names and encoders info\n",
    "            feature_info = {\n",
    "                'features': list(X_train_processed.columns),\n",
    "                'n_features': X_train_processed.shape[1],\n",
    "                'encoded_columns': list(label_encoders.keys()) if 'label_encoders' in locals() else []\n",
    "            }\n",
    "            \n",
    "            # Create temporary file\n",
    "            temp_file = 'feature_info.json'\n",
    "            with open(temp_file, 'w') as f:\n",
    "                json.dump(feature_info, f, indent=2)\n",
    "            \n",
    "            # Try to log artifact\n",
    "            mlflow.log_artifact(temp_file)\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "            \n",
    "            print(\"✓ Additional artifacts logged\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not log additional artifacts: {e}\")\n",
    "            print(\"  This is usually due to S3 bucket permissions or connectivity issues.\")\n",
    "            \n",
    "            # Clean up temporary file even if logging failed\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Log Tags\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"Logging tags...\")\n",
    "        \n",
    "        try:\n",
    "            mlflow.set_tag(\"model_family\", \"fraud_detection\")\n",
    "            mlflow.set_tag(\"data_source\", inputDBFormat)\n",
    "            mlflow.set_tag(\"best_model\", \"true\")\n",
    "            mlflow.set_tag(\"production_ready\", \"true\")\n",
    "            mlflow.set_tag(\"preprocessing\", \"manual\")\n",
    "            \n",
    "            print(\"✓ Tags logged\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not log tags: {e}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # Display Run Information\n",
    "        # ====================================================================\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"MLFLOW RUN SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        print(f\"Experiment ID: {run.info.experiment_id}\")\n",
    "        print(f\"Model: {best_model_name}\")\n",
    "        print(f\"Test F1 Score: {best_result['test_f1']:.4f}\")\n",
    "        print(f\"Test Accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "        print()\n",
    "        print(f\"View run at: http://localhost:4000/#/experiments/{experiment.experiment_id}/runs/{run.info.run_id}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    print()\n",
    "    print(\"✅ MLflow logging complete!\")\n",
    "    print()\n",
    "    print(\"Note: If you see warnings about artifacts or model logging,\")\n",
    "    print(\"this is usually due to S3 bucket configuration. Metrics and\")\n",
    "    print(\"parameters are always logged to the MLflow tracking server.\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No trained models found. Please run the model training cell first.\")\n",
    "    print(\"Variables 'best_model_name' and 'best_result' are required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u8lu48qhqat",
   "metadata": {},
   "source": [
    "## Exporting model to a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wr1fts6niq",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "separator = \"=\" * 80\n",
    "print(separator)\n",
    "print(\"UPLOAD MODEL TO S3 BUCKET\")\n",
    "print(separator)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# S3 Configuration\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "load_dotenv(r\"..\\Architecture\\localModel\\.env\")\n",
    "\n",
    "AWS_REGION = os.getenv(\"AWS_DEFAULT_REGION\")  # Default to eu-west-3 if not set\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\")  # Ensure this is set in your environment\n",
    "S3_PREFIX = os.getenv(\"S3_PREFIX\")  #\n",
    "S3_USERNAME = os.getenv(\"AWS_ACCESS_KEY_ID\")  # Not used in this script but can be for custom paths\n",
    "S3_PASSWORD = os.getenv(\"AWS_SECRET_ACCESS_KEY\")  # Not used in this script but can be for custom paths\n",
    "print(f\"S3 Bucket: s3://{S3_BUCKET_NAME}/{S3_PREFIX}\")\n",
    "print(f\"AWS Region: {AWS_REGION}\")\n",
    "print()\n",
    "\n",
    "# Check if model exists\n",
    "if 'best_model_name' not in locals() or 'best_result' not in locals():\n",
    "    print(\"⚠️ No trained model found. Please run the model training cell first.\")\n",
    "else:\n",
    "    print(f\"Preparing to upload {best_model_name} to S3...\")\n",
    "    print(f\"Bucket: s3://{S3_BUCKET_NAME}/{S3_PREFIX}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Install boto3 if not already installed\n",
    "        import boto3\n",
    "        from botocore.exceptions import ClientError, NoCredentialsError\n",
    "        \n",
    "        print(\"✓ boto3 library loaded\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Installing boto3...\")\n",
    "        %pip install boto3 --quiet\n",
    "        import boto3\n",
    "        from botocore.exceptions import ClientError, NoCredentialsError\n",
    "        print(\"✓ boto3 installed and loaded\")\n",
    "    \n",
    "    import joblib\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    # ========================================================================\n",
    "    # Save Model Locally First\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 1: Saving model locally...\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    models_dir = \"models\"\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_filename = f\"{best_model_name}_fraud_detector_{timestamp}.pkl\"\n",
    "    model_path = os.path.join(models_dir, model_filename)\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_result['model'], model_path)\n",
    "    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"✓ Model saved locally: {model_path}\")\n",
    "    print(f\"  File size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save Model Metadata\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 2: Creating metadata file...\")\n",
    "    \n",
    "    metadata = {\n",
    "        \"model_name\": best_model_name,\n",
    "        \"model_type\": str(type(best_result['model']).__name__),\n",
    "        \"timestamp\": timestamp,\n",
    "        \"metrics\": {\n",
    "            \"test_accuracy\": float(best_result['test_accuracy']),\n",
    "            \"test_f1_score\": float(best_result['test_f1']),\n",
    "            \"train_accuracy\": float(best_result['train_accuracy']),\n",
    "            \"train_f1_score\": float(best_result['train_f1']),\n",
    "            \"training_time_seconds\": float(best_result['train_time'])\n",
    "        },\n",
    "        \"dataset_info\": {\n",
    "            \"total_samples\": int(len(X)),\n",
    "            \"train_samples\": int(len(X_train)),\n",
    "            \"test_samples\": int(len(X_test)),\n",
    "            \"fraud_ratio\": float(Y.sum() / len(Y)),\n",
    "            \"n_features\": int(X_train_processed.shape[1])\n",
    "        },\n",
    "        \"features\": list(X_train_processed.columns),\n",
    "        \"encoded_columns\": list(label_encoders.keys()) if 'label_encoders' in locals() else []\n",
    "    }\n",
    "    \n",
    "    metadata_filename = f\"{best_model_name}_metadata_{timestamp}.json\"\n",
    "    metadata_path = os.path.join(models_dir, metadata_filename)\n",
    "    \n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Metadata saved: {metadata_path}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Save Label Encoders\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 3: Saving label encoders...\")\n",
    "    \n",
    "    if 'label_encoders' in locals() and label_encoders:\n",
    "        encoders_filename = f\"label_encoders_{timestamp}.pkl\"\n",
    "        encoders_path = os.path.join(models_dir, encoders_filename)\n",
    "        \n",
    "        joblib.dump(label_encoders, encoders_path)\n",
    "        print(f\"✓ Label encoders saved: {encoders_path}\")\n",
    "    else:\n",
    "        encoders_path = None\n",
    "        print(\"⊘ No label encoders to save\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Upload to S3\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nStep 4: Uploading to S3...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Initialize S3 client\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            region_name=AWS_REGION,\n",
    "            aws_access_key_id=S3_USERNAME,\n",
    "            aws_secret_access_key=S3_PASSWORD\n",
    "        )\n",
    "        \n",
    "        # Test connection by checking if bucket exists\n",
    "        try:\n",
    "            s3_client.head_bucket(Bucket=S3_BUCKET_NAME)\n",
    "            print(f\"✓ S3 bucket '{S3_BUCKET_NAME}' is accessible\")\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            if error_code == '404':\n",
    "                print(f\"⚠️ Bucket '{S3_BUCKET_NAME}' does not exist\")\n",
    "                print(\"  Creating bucket...\")\n",
    "                s3_client.create_bucket(\n",
    "                    Bucket=S3_BUCKET_NAME,\n",
    "                    CreateBucketConfiguration={'LocationConstraint': AWS_REGION}\n",
    "                )\n",
    "                print(f\"✓ Bucket created successfully\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Upload model file\n",
    "        s3_model_key = f\"{S3_PREFIX}/models/{model_filename}\"\n",
    "        print(f\"\\nUploading model to s3://{S3_BUCKET_NAME}/{s3_model_key}\")\n",
    "        s3_client.upload_file(\n",
    "            model_path, \n",
    "            S3_BUCKET_NAME, \n",
    "            s3_model_key,\n",
    "            ExtraArgs={'ContentType': 'application/octet-stream'}\n",
    "        )\n",
    "        print(f\"✓ Model uploaded successfully\")\n",
    "        \n",
    "        # Upload metadata\n",
    "        s3_metadata_key = f\"{S3_PREFIX}/metadata/{metadata_filename}\"\n",
    "        print(f\"\\nUploading metadata to s3://{S3_BUCKET_NAME}/{s3_metadata_key}\")\n",
    "        s3_client.upload_file(\n",
    "            metadata_path, \n",
    "            S3_BUCKET_NAME, \n",
    "            s3_metadata_key,\n",
    "            ExtraArgs={'ContentType': 'application/json'}\n",
    "        )\n",
    "        print(f\"✓ Metadata uploaded successfully\")\n",
    "        \n",
    "        # Upload encoders if they exist\n",
    "        if encoders_path:\n",
    "            s3_encoders_key = f\"{S3_PREFIX}/encoders/{encoders_filename}\"\n",
    "            print(f\"\\nUploading encoders to s3://{S3_BUCKET_NAME}/{s3_encoders_key}\")\n",
    "            s3_client.upload_file(\n",
    "                encoders_path, \n",
    "                S3_BUCKET_NAME, \n",
    "                s3_encoders_key,\n",
    "                ExtraArgs={'ContentType': 'application/octet-stream'}\n",
    "            )\n",
    "            print(f\"✓ Encoders uploaded successfully\")\n",
    "        \n",
    "        # Generate presigned URLs (valid for 7 days)\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"UPLOAD SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nModel uploaded to S3!\")\n",
    "        print(f\"Bucket: {S3_BUCKET_NAME}\")\n",
    "        print(f\"Prefix: {S3_PREFIX}\")\n",
    "        print()\n",
    "        print(\"Files uploaded:\")\n",
    "        print(f\"  • Model:    s3://{S3_BUCKET_NAME}/{s3_model_key}\")\n",
    "        print(f\"  • Metadata: s3://{S3_BUCKET_NAME}/{s3_metadata_key}\")\n",
    "        if encoders_path:\n",
    "            print(f\"  • Encoders: s3://{S3_BUCKET_NAME}/{s3_encoders_key}\")\n",
    "        print()\n",
    "        \n",
    "        # Generate presigned URL for model download\n",
    "        try:\n",
    "            model_url = s3_client.generate_presigned_url(\n",
    "                'get_object',\n",
    "                Params={'Bucket': S3_BUCKET_NAME, 'Key': s3_model_key},\n",
    "                ExpiresIn=604800  # 7 days\n",
    "            )\n",
    "            print(\"Presigned URL (valid for 7 days):\")\n",
    "            print(model_url)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not generate presigned URL: {e}\")\n",
    "        \n",
    "        print(\"\\n✅ Upload to S3 completed successfully!\")\n",
    "        \n",
    "    except NoCredentialsError:\n",
    "        print(\"\\n❌ AWS credentials not found!\")\n",
    "        print(\"\\nPlease configure AWS credentials using one of these methods:\")\n",
    "        print(\"\\n1. AWS CLI:\")\n",
    "        print(\"   aws configure\")\n",
    "        print(\"\\n2. Environment variables:\")\n",
    "        print(\"   export AWS_ACCESS_KEY_ID='your-access-key'\")\n",
    "        print(\"   export AWS_SECRET_ACCESS_KEY='your-secret-key'\")\n",
    "        print(\"   export AWS_DEFAULT_REGION='eu-west-3'\")\n",
    "        print(\"\\n3. IAM Role (if running on EC2/Lambda)\")\n",
    "        print(\"\\nLocal files are still saved in:\", models_dir)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        print(f\"\\n❌ S3 Error ({error_code}): {e.response['Error']['Message']}\")\n",
    "        print(\"\\nPossible issues:\")\n",
    "        print(\"  • Check bucket name is correct\")\n",
    "        print(\"  • Verify AWS credentials have S3 permissions\")\n",
    "        print(\"  • Ensure region is correct\")\n",
    "        print(\"\\nLocal files are still saved in:\", models_dir)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Unexpected error: {str(e)}\")\n",
    "        print(\"\\nLocal files are still saved in:\", models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08a1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
