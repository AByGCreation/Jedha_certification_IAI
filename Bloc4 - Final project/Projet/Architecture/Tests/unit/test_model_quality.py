import os
import sys
from pathlib import Path
import pytest
from fastapi.testclient import TestClient
import json
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv, find_dotenv
import warnings
import logging
import mlflow
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import pandas as pd

warnings.filterwarnings("ignore", category=DeprecationWarning)
# Add the parent directory to the path so src module can be imported
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
sys.path.append('./Bloc4 - Final project/Projet/Architecture')

#important ! ne pas deplacer avant le sys.path.append
#from App.Dockers.fastapi.main import app, getMyModel, getModelRunID, load_reference_dataset

from App.Dockers.fastapi.main import app, getMyModel, load_reference_dataset, Preprocessor

# Trouve le fichier .env
dotenv_path = find_dotenv()
load_dotenv(".env")  # Depuis le rÃ©pertoire courant

print(f"âœ… .env chargÃ© dans test_predict.py")
print(f"   TesVAr : {os.getenv('EXPERIMENT_NAME')}")
print(f"Fichier .env trouvÃ© : {dotenv_path}")
EXPERIMENT_NAME = "LBPFraudDetector"

#=======================================
# Test de la qualitÃ© du modÃ¨le MLflow
#=======================================

class TestModelQuality:
    """Test de qualitÃ© du modÃ¨le MLflow"""
    
    @pytest.fixture(scope="class", autouse=True)
    def setup_model(self):
        """Initialize model before tests"""
        print("\nðŸ¤– Loading model for quality tests...")
        model = getMyModel()
        
        if model is None:
            pytest.exit("âŒ Failed to load model")
        
        app.state.loaded_model = model
        print("âœ… Model loaded for quality testing")
        yield
    
    def test_model_accuracy_threshold(self):
        """Test 1: VÃ©rifier que la prÃ©cision du modÃ¨le dÃ©passe 80%"""
        
        try:
            # Load reference dataset
            X_test, y_test = load_reference_dataset()
            
            print(f"\nðŸ“Š Testing on reference dataset:")
            print(f"   - Shape: {X_test.shape}")
            print(f"   - Columns: {X_test.columns.tolist()}")
            
            # Preprocess test data (important!)
            X_test_preprocessed = Preprocessor(X_test.copy())
            
            print(f"   - After preprocessing: {X_test_preprocessed.shape}")
            print(f"   - Columns after preprocessing: {X_test_preprocessed.columns.tolist()}")
            
            # Make predictions
            model = app.state.loaded_model
            predictions = model.predict(X_test_preprocessed)
            
            # Calculate metrics
            accuracy = accuracy_score(y_test, predictions)
            precision = precision_score(y_test, predictions, zero_division=0)
            recall = recall_score(y_test, predictions, zero_division=0)
            f1 = f1_score(y_test, predictions, zero_division=0)
            
            print(f"\nðŸ“ˆ Model Performance Metrics:")
            print(f"   - Accuracy:  {accuracy:.4f}")
            print(f"   - Precision: {precision:.4f}")
            print(f"   - Recall:    {recall:.4f}")
            print(f"   - F1-Score:  {f1:.4f}")
            
            # Assertions
            assert accuracy >= 0.75, f"Accuracy {accuracy:.4f} is below threshold 0.75"
            assert precision >= 0.70, f"Precision {precision:.4f} is below threshold 0.70"
            assert recall >= 0.60, f"Recall {recall:.4f} is below threshold 0.60"
            assert f1 >= 0.65, f"F1-Score {f1:.4f} is below threshold 0.65"
            
            print(f"âœ… All quality metrics passed!")
            
        except Exception as e:
            print(f"âŒ Model quality test failed: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def test_model_prediction_consistency(self):
        """Test 2: VÃ©rifier que le modÃ¨le donne les mÃªmes prÃ©dictions pour les mÃªmes entrÃ©es"""
        
        try:
            # Load dataset
            X_test, y_test = load_reference_dataset()
            
            # Preprocess
            X_test_preprocessed = Preprocessor(X_test.copy())
            
            # Get subset for test (first 100 samples)
            X_subset = X_test_preprocessed.head(100)
            
            model = app.state.loaded_model
            
            # Make two predictions
            predictions_1 = model.predict(X_subset)
            predictions_2 = model.predict(X_subset)
            
            # Check consistency
            is_consistent = np.array_equal(predictions_1, predictions_2)
            
            print(f"\nðŸ”„ Prediction Consistency Test:")
            print(f"   - First run:  {predictions_1[:5]}...")
            print(f"   - Second run: {predictions_2[:5]}...")
            print(f"   - Consistent: {is_consistent}")
            
            assert is_consistent, "Model predictions are not consistent"
            print(f"âœ… Model predictions are consistent!")
            
        except Exception as e:
            print(f"âŒ Consistency test failed: {str(e)}")
            raise
    
    def test_model_output_format(self):
        """Test 3: VÃ©rifier que le modÃ¨le retourne le bon format de sortie"""
        
        try:
            # Load dataset
            X_test, y_test = load_reference_dataset()
            
            # Preprocess
            X_test_preprocessed = Preprocessor(X_test.copy())
            
            # Get subset
            X_subset = X_test_preprocessed.head(10)
            
            model = app.state.loaded_model
            predictions = model.predict(X_subset)
            
            print(f"\nðŸ“‹ Output Format Test:")
            print(f"   - Predictions type: {type(predictions)}")
            print(f"   - Predictions shape: {predictions.shape}")
            print(f"   - Predictions dtype: {predictions.dtype}")
            print(f"   - Unique values: {np.unique(predictions)}")
            
            # Assertions
            assert isinstance(predictions, np.ndarray), "Predictions should be numpy array"
            assert len(predictions) == len(X_subset), "Predictions count should match input"
            assert predictions.dtype in [np.int32, np.int64, np.float32, np.float64], "Invalid dtype"
            assert all(p in [0, 1] for p in predictions), "Predictions should be binary (0 or 1)"
            
            print(f"âœ… Output format is correct!")
            
        except Exception as e:
            print(f"âŒ Output format test failed: {str(e)}")
            raise


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])



















#==========================================
#==========================================


# def test_model_accuracy_threshold():
#     """
#     Seuil mÃ©tier : Accuracy â‰¥ 92%
#     Justification : BasÃ© sur analyse coÃ»t/bÃ©nÃ©fice
#     - Faux nÃ©gatif (fraude non dÃ©tectÃ©e) : 150â‚¬ de perte moyenne
#     - Faux positif (client bloquÃ©) : 5â‚¬ de friction + risque churn


#     model_uri: str = MODEL_URI,

    
#     Charge un modÃ¨le MLflow (sklearn) Ã  partir d'un tracking URI et d'un model URI.
#     Par dÃ©faut : modÃ¨le RF hotel_cancellation_detector_RF.
#     """

#     modelRunID = getModelRunID()
#     print(f"Model runid: {modelRunID.latest_versions[0].run_id}")
#     print(f"Model name: {modelRunID.latest_versions[0].name}")

#     model_uri = os.getenv("MODEL_URI", f"runs:/{modelRunID.latest_versions[0].run_id}/{EXPERIMENT_NAME}")
#     tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
   

#     print(f"ðŸ” Chargement du modÃ¨le depuis MLflow : {model_uri} @ {tracking_uri}")


#     mlflow.set_tracking_uri(tracking_uri)
#     model = mlflow.sklearn.load_model(model_uri)
#     logging.info("âœ… Model rÃ©cupÃ©rÃ© depuis MLflow")
#     X_test, y_test = load_reference_dataset()
    
#     y_pred = model.predict(X_test)
#     accuracy = accuracy_score(y_test, y_pred)
    
#     assert accuracy >= 0.92, f"Accuracy trop faible: {accuracy:.3f}"



#     # model = mlflow.sklearn.load_model("models:/fraud-detection/production")
#     # X_test, y_test = load_reference_dataset()
    
#     # y_pred = model.predict(X_test)
#     # accuracy = accuracy_score(y_test, y_pred)
    
#     # assert accuracy >= 0.92, f"Accuracy trop faible: {accuracy:.3f}"




# def test_model_f1_score_threshold():
#     """
#     Seuil mÃ©tier : F1-Score â‰¥ 85%
#     Justification : Ã‰quilibre prÃ©cision/rappel critique en dÃ©tection fraude
#     """
#     f1 = f1_score(y_test, y_pred)
#     assert f1 >= 0.85, f"F1-score trop faible: {f1:.3f}"